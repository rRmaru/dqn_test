{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "base (Python 3.10.8) に接続されました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode\tepsilon\treward\ttotal_step\ttime\n",
      "1000\t0.0999999999999992\t-200.0\t200000\t288.44751715660095[sec]\n",
      "2000\t0.0999999999999992\t-200.0\t400000\t289.387996673584[sec]\n",
      "3000\t0.0999999999999992\t-200.0\t600000\t290.02379989624023[sec]\n",
      "4000\t0.0999999999999992\t-200.0\t800000\t289.3160035610199[sec]\n",
      "5000\t0.0999999999999992\t-200.0\t1000000\t290.0819492340088[sec]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as option\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import settings\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "#make Generator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "#make environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "obs_num = env.observation_space.shape[0]\n",
    "act_num = env.action_space.n\n",
    "\n",
    "#make Neural Network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_num, settings.HIDDEN_SIZE)\n",
    "        self.fc2 = nn.Linear(settings.HIDDEN_SIZE, settings.HIDDEN_SIZE)\n",
    "        self.fc3 = nn.Linear(settings.HIDDEN_SIZE, settings.HIDDEN_SIZE)\n",
    "        self.fc4 = nn.Linear(settings.HIDDEN_SIZE, act_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        y = F.relu(self.fc4(x))\n",
    "        return y\n",
    "        \n",
    "def main():\n",
    "    Q_train = NN()\n",
    "    Q_target = copy.deepcopy(Q_train)\n",
    "    optimizer = option.RMSprop(Q_train.parameters(), lr=0.00015, alpha=0.95, eps=0.01)  #最適化\n",
    "    \n",
    "    total_step = 0\n",
    "    memory = ReplayBuffer(settings.BAFFER_SIZE)\n",
    "    total_rewards = []\n",
    "    \n",
    "    #学習開始\n",
    "    print(\"\\t\".join([\"episode\", \"epsilon\", \"reward\", \"total_step\", \"time\"]))\n",
    "    start = time.time()\n",
    "    \n",
    "    for episode in range(settings.EPISODE_NUM):\n",
    "        pobs, _ = env.reset()\n",
    "        step = 0        #step\n",
    "        done = False    #judge end game\n",
    "        total_reward = 0   #累積報酬\n",
    "        while not done and (step < settings.STEP_MAX):\n",
    "            #行動選択(適当な行動値)\n",
    "            act = env.action_space.sample()\n",
    "            # ε-greedy法\n",
    "            if rng.random() > settings.EPSILON:\n",
    "                pobs_ = np.array(pobs, dtype=\"float32\").reshape((1, obs_num))\n",
    "                pobs_ = Variable(torch.from_numpy(pobs_))\n",
    "                act = Q_train(pobs_)\n",
    "                max, indices = torch.max(act.data, 1)  #valueとindicesが返ってくる\n",
    "                act = indices.numpy()[0]\n",
    "\n",
    "            #実行\n",
    "            obs, reward, done, _, _ = env.step(act)\n",
    "            \n",
    "            # update reward 1\n",
    "            if obs[0] > -0.2 :\n",
    "                reward = 0.5\n",
    "            \n",
    "            # update reward 2\n",
    "            if obs[0] > 0.5:\n",
    "                reward = 100\n",
    "            #add memory\n",
    "            memory.add((pobs, act, reward, obs, done))  #次状態、行動、報酬、状態、エピソード終了判定をbufferに格納\n",
    "            \n",
    "            \n",
    "            #学習\n",
    "            if len(memory) == settings.BAFFER_SIZE:\n",
    "                if total_step % settings.TRAIN_FREQ == 0:\n",
    "                    for i in range(int(settings.BAFFER_SIZE/settings.BATCH_SIZE)):\n",
    "                        batch = memory.sample(settings.BATCH_SIZE)\n",
    "                        pobss = np.array([b[0] for b in batch], dtype=\"float32\").reshape((settings.BATCH_SIZE, obs_num))\n",
    "                        acts = np.array([b[1] for b in batch], dtype=\"int\")\n",
    "                        rewards = np.array([b[2] for b in batch], dtype=\"float32\")\n",
    "                        obss = np.array([b[3] for b in batch], dtype=\"float32\").reshape((settings.BATCH_SIZE, obs_num))\n",
    "                        dones = np.array([b[4] for b in batch], dtype=\"float32\")\n",
    "                        \n",
    "                        #set y\n",
    "                        pobss_ = Variable(torch.from_numpy(pobss))\n",
    "                        q = Q_train(pobss_)\n",
    "                        obss_ = Variable(torch.from_numpy(obss))\n",
    "                        maxs, indices = torch.max(Q_target(obss_).data, 1)\n",
    "                        maxq = maxs.numpy() #maxQ\n",
    "                        target = copy.deepcopy(q.data.numpy())\n",
    "                        for j in range(settings.BATCH_SIZE):\n",
    "                            target[j, acts[j]] = rewards[j]+settings.GAMMA*maxq[j]*(not dones[j])    #教師信号\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = nn.MSELoss()(q, Variable(torch.from_numpy(target)))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                #Q関数の更新\n",
    "                if total_step % settings.UPDATE_TARGET_Q_FREQ == 0:\n",
    "                    Q_target = copy.deepcopy(Q_train)\n",
    "            #εの減少\n",
    "            if settings.EPSILON > settings.EPSILON_MIN and total_step > settings.START_REDUCE_EPSILON:\n",
    "                settings.EPSILON -= settings.EPSILON_DECREASE\n",
    "                \n",
    "            #次の行動へ\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            pobs = obs       \n",
    "            \n",
    "        total_rewards.append(total_reward)  #累積報酬を記録\n",
    "        \n",
    "        if(episode + 1) % settings.LOG_FREQ == 0:\n",
    "            r = sum(total_rewards[((episode + 1) - settings.LOG_FREQ):(episode + 1)])/settings.LOG_FREQ\n",
    "            elapsed_time = time.time() - start\n",
    "            print(\"\\t\".join(map(str, [episode + 1, settings.EPSILON, r, total_step, str(elapsed_time)+\"[sec]\"])))\n",
    "            start = time.time()\n",
    "            \n",
    "\n",
    "            \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "base (Python 3.10.8) に接続されました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dqn_test/CartPole_test/train.py\u001b[0m in \u001b[0;36mline 91\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=87'>88</a>\u001b[0m         q \u001b[39m=\u001b[39m main_net(torch\u001b[39m.\u001b[39mtensor(state[\u001b[39mNone\u001b[39;00m, :], device\u001b[39m=\u001b[39mdevice))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=88'>89</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> <a href='file:///home/dqn_test/CartPole_test/train.py?line=90'>91</a>\u001b[0m next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=91'>92</a>\u001b[0m ep_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=92'>93</a>\u001b[0m reward \u001b[39m=\u001b[39m reward \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mreward\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TOTAL_TS = 10000\n",
    "SYNC_FREQ = 10\n",
    "seed = 2023\n",
    "\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def append(self, obj):\n",
    "        if self.size() > self.buffer_size:\n",
    "            print('buffer size larger than set value, trimming...')\n",
    "            self.buffer = self.buffer[(self.size() - self.buffer_size):]\n",
    "        elif self.size() == self.buffer_size:\n",
    "            self.buffer[self.index] = obj\n",
    "            self.index += 1\n",
    "            self.index %= self.buffer_size\n",
    "        else:\n",
    "            self.buffer.append(obj)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size, device=\"cpu\"):\n",
    "        if self.size() < batch_size:\n",
    "            batch = random.sample(self.buffer, self.size())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        res = []\n",
    "        for i in range(5):\n",
    "            k = np.stack(tuple(item[i] for item in batch), axis=0)\n",
    "            res.append(torch.tensor(k, device=device))\n",
    "        return res[0], res[1], res[2], res[3], res[4]\n",
    "\n",
    "device = \"cuda\" # \"cpu\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "buffer = ReplayBuffer(buffer_size=MEMORY_SIZE)\n",
    "main_net = nn.Sequential(\n",
    "    nn.Linear(n_observations, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 128), nn.ReLU(),\n",
    "    nn.Linear(128, n_actions)\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(main_net.parameters())\n",
    "\n",
    "from copy import deepcopy\n",
    "target_net = deepcopy(main_net)\n",
    "\n",
    "num_episode = global_ts = 0\n",
    "ep_rewards = list()\n",
    "while global_ts <= TOTAL_TS:\n",
    "    state, done = env.reset(), False\n",
    "    ep_reward, ep_ts = 0.0, 0\n",
    "    while not done:\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * global_ts / EPS_DECAY)\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.choice(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = main_net(torch.tensor(state[None, :], device=device)).detach().cpu().numpy()\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        reward = reward if not done else -reward\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            ep_rewards.append(ep_reward)\n",
    "            if (num_episode == 0) or ((num_episode + 1) % 50 == 0):\n",
    "                print(f\"Ep-{num_episode + 1} {global_ts + 1}/{TOTAL_TS} Eps: {eps:.2f}, Reward: {ep_reward}\")\n",
    "            break\n",
    "        \n",
    "        if buffer.size() >= BATCH_SIZE:\n",
    "            obses_t, actions, rewards, obses_tp1, dones = buffer.sample(BATCH_SIZE, device)\n",
    "            # Optimize the model\n",
    "            with torch.no_grad():\n",
    "                target_q = target_net(obses_tp1).detach().max(1)[0]\n",
    "            target = rewards + GAMMA * target_q * (1 - dones.float())\n",
    "            val_t = main_net(obses_t).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            loss = F.mse_loss(val_t.float(), target.float())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Periodically sync the target and main networks\n",
    "            if (global_ts + 1) % SYNC_FREQ == 0:\n",
    "                target_net.load_state_dict(main_net.state_dict())\n",
    "        ep_ts += 1\n",
    "        global_ts += 1\n",
    "    num_episode += 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_average(a, n=5) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.plot(moving_average(ep_rewards))\n",
    "plt.xlabel(\"#Episode\")\n",
    "plt.ylabel(\"Ep-Return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep-1 16/10000 Eps: 0.99, Reward: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arrays = [asanyarray(arr) for arr in arrays]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dqn_test/CartPole_test/train.py\u001b[0m in \u001b[0;36mline 103\n\u001b[1;32m    <a href='file:///home/dqn_test/CartPole_test/train.py?line=99'>100</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dqn_test/CartPole_test/train.py?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m buffer\u001b[39m.\u001b[39msize() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m BATCH_SIZE:\n\u001b[0;32m--> <a href='file:///home/dqn_test/CartPole_test/train.py?line=102'>103</a>\u001b[0m     obses_t, actions, rewards, obses_tp1, dones \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39;49msample(BATCH_SIZE, device)\n\u001b[1;32m    <a href='file:///home/dqn_test/CartPole_test/train.py?line=103'>104</a>\u001b[0m     \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dqn_test/CartPole_test/train.py?line=104'>105</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m/home/dqn_test/CartPole_test/train.py\u001b[0m in \u001b[0;36mline 57\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size, device)\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=54'>55</a>\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m---> <a href='file:///home/dqn_test/CartPole_test/train.py?line=56'>57</a>\u001b[0m     k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack(\u001b[39mtuple\u001b[39;49m(item[i] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m batch), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=57'>58</a>\u001b[0m     res\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(k, device\u001b[39m=\u001b[39mdevice))\n\u001b[1;32m     <a href='file:///home/dqn_test/CartPole_test/train.py?line=58'>59</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m], res[\u001b[39m2\u001b[39m], res[\u001b[39m3\u001b[39m], res[\u001b[39m4\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:426\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=423'>424</a>\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=424'>425</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=425'>426</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=427'>428</a>\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=428'>429</a>\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TOTAL_TS = 10000\n",
    "SYNC_FREQ = 10\n",
    "seed = 2023\n",
    "\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def append(self, obj):\n",
    "        if self.size() > self.buffer_size:\n",
    "            print('buffer size larger than set value, trimming...')\n",
    "            self.buffer = self.buffer[(self.size() - self.buffer_size):]\n",
    "        elif self.size() == self.buffer_size:\n",
    "            self.buffer[self.index] = obj\n",
    "            self.index += 1\n",
    "            self.index %= self.buffer_size\n",
    "        else:\n",
    "            self.buffer.append(obj)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size, device=\"cpu\"):\n",
    "        if self.size() < batch_size:\n",
    "            batch = random.sample(self.buffer, self.size())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        res = []\n",
    "        for i in range(5):\n",
    "            k = np.stack(tuple(item[i] for item in batch), axis=0)\n",
    "            res.append(torch.tensor(k, device=device))\n",
    "        return res[0], res[1], res[2], res[3], res[4]\n",
    "\n",
    "device = \"cuda\" # \"cpu\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "buffer = ReplayBuffer(buffer_size=MEMORY_SIZE)\n",
    "main_net = nn.Sequential(\n",
    "    nn.Linear(n_observations, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 128), nn.ReLU(),\n",
    "    nn.Linear(128, n_actions)\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(main_net.parameters())\n",
    "\n",
    "from copy import deepcopy\n",
    "target_net = deepcopy(main_net)\n",
    "\n",
    "num_episode = global_ts = 0\n",
    "ep_rewards = list()\n",
    "while global_ts <= TOTAL_TS:\n",
    "    state, done = env.reset(), False\n",
    "    ep_reward, ep_ts = 0.0, 0\n",
    "    while not done:\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * global_ts / EPS_DECAY)\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.choice(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = main_net(torch.tensor(state[None, :], device=device)).detach().cpu().numpy()\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        reward = reward if not done else -reward\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            ep_rewards.append(ep_reward)\n",
    "            if (num_episode == 0) or ((num_episode + 1) % 50 == 0):\n",
    "                print(f\"Ep-{num_episode + 1} {global_ts + 1}/{TOTAL_TS} Eps: {eps:.2f}, Reward: {ep_reward}\")\n",
    "            break\n",
    "        \n",
    "        if buffer.size() >= BATCH_SIZE:\n",
    "            obses_t, actions, rewards, obses_tp1, dones = buffer.sample(BATCH_SIZE, device)\n",
    "            # Optimize the model\n",
    "            with torch.no_grad():\n",
    "                target_q = target_net(obses_tp1).detach().max(1)[0]\n",
    "            target = rewards + GAMMA * target_q * (1 - dones.float())\n",
    "            val_t = main_net(obses_t).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            loss = F.mse_loss(val_t.float(), target.float())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Periodically sync the target and main networks\n",
    "            if (global_ts + 1) % SYNC_FREQ == 0:\n",
    "                target_net.load_state_dict(main_net.state_dict())\n",
    "        ep_ts += 1\n",
    "        global_ts += 1\n",
    "    num_episode += 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_average(a, n=5) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.plot(moving_average(ep_rewards))\n",
    "plt.xlabel(\"#Episode\")\n",
    "plt.ylabel(\"Ep-Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "base (Python 3.10.8) に接続されました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep-1 12/10000 Eps: 0.99, Reward: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arrays = [asanyarray(arr) for arr in arrays]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m buffer\u001b[39m.\u001b[39msize() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m BATCH_SIZE:\n\u001b[0;32m--> 103\u001b[0m     obses_t, actions, rewards, obses_tp1, dones \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39;49msample(BATCH_SIZE, device)\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack(\u001b[39mtuple\u001b[39;49m(item[i] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m batch), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     58\u001b[0m     res\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(k, device\u001b[39m=\u001b[39mdevice))\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m res[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m], res[\u001b[39m2\u001b[39m], res[\u001b[39m3\u001b[39m], res[\u001b[39m4\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:426\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=423'>424</a>\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=424'>425</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=425'>426</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=427'>428</a>\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py?line=428'>429</a>\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TOTAL_TS = 10000\n",
    "SYNC_FREQ = 10\n",
    "seed = 2023\n",
    "\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def append(self, obj):\n",
    "        if self.size() > self.buffer_size:\n",
    "            print('buffer size larger than set value, trimming...')\n",
    "            self.buffer = self.buffer[(self.size() - self.buffer_size):]\n",
    "        elif self.size() == self.buffer_size:\n",
    "            self.buffer[self.index] = obj\n",
    "            self.index += 1\n",
    "            self.index %= self.buffer_size\n",
    "        else:\n",
    "            self.buffer.append(obj)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size, device=\"cpu\"):\n",
    "        if self.size() < batch_size:\n",
    "            batch = random.sample(self.buffer, self.size())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        res = []\n",
    "        for i in range(5):\n",
    "            k = np.stack(tuple(item[i] for item in batch), axis=0)\n",
    "            res.append(torch.tensor(k, device=device))\n",
    "        return res[0], res[1], res[2], res[3], res[4]\n",
    "\n",
    "device = \"cuda\" # \"cpu\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "buffer = ReplayBuffer(buffer_size=MEMORY_SIZE)\n",
    "main_net = nn.Sequential(\n",
    "    nn.Linear(n_observations, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 128), nn.ReLU(),\n",
    "    nn.Linear(128, n_actions)\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(main_net.parameters())\n",
    "\n",
    "from copy import deepcopy\n",
    "target_net = deepcopy(main_net)\n",
    "\n",
    "num_episode = global_ts = 0\n",
    "ep_rewards = list()\n",
    "while global_ts <= TOTAL_TS:\n",
    "    state, done = env.reset(), False\n",
    "    ep_reward, ep_ts = 0.0, 0\n",
    "    while not done:\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * global_ts / EPS_DECAY)\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.choice(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = main_net(torch.tensor(state[None, :], device=device)).detach().cpu().numpy()\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        reward = reward if not done else -reward\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            ep_rewards.append(ep_reward)\n",
    "            if (num_episode == 0) or ((num_episode + 1) % 50 == 0):\n",
    "                print(f\"Ep-{num_episode + 1} {global_ts + 1}/{TOTAL_TS} Eps: {eps:.2f}, Reward: {ep_reward}\")\n",
    "            break\n",
    "        \n",
    "        if buffer.size() >= BATCH_SIZE:\n",
    "            obses_t, actions, rewards, obses_tp1, dones = buffer.sample(BATCH_SIZE, device)\n",
    "            # Optimize the model\n",
    "            with torch.no_grad():\n",
    "                target_q = target_net(obses_tp1).detach().max(1)[0]\n",
    "            target = rewards + GAMMA * target_q * (1 - dones.float())\n",
    "            val_t = main_net(obses_t).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            loss = F.mse_loss(val_t.float(), target.float())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Periodically sync the target and main networks\n",
    "            if (global_ts + 1) % SYNC_FREQ == 0:\n",
    "                target_net.load_state_dict(main_net.state_dict())\n",
    "        ep_ts += 1\n",
    "        global_ts += 1\n",
    "    num_episode += 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_average(a, n=5) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.plot(moving_average(ep_rewards))\n",
    "plt.xlabel(\"#Episode\")\n",
    "plt.ylabel(\"Ep-Return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
